# === TIPS AND TRICKS TO IMPROVE PERFORMANCE ===

tcp_nodelay on;     # sets TCP_NODELAY flag, used on keepalive connections
# Nagle’s algorithm aims to prevent being overwhelmed by a great number of small packets. 
# It does not interfere with full-size TCP packets (Maximum Segment Size, or MSS in short), 
# only with packets that are smaller than MSS. Those packages will be transmitted only if 
# the receiver successfully sends back all the acknowledegments of previous packages (ACKs). 
# And during the wait, the sender can buffer more data.

sendfile on;
# Normally, when a file needs to be sent, the following steps are required:

# malloc(3): allocate a local buffer for storing object data
# read(2): retrieve and copy the object into the local buffer
# write(2): copy the object from the local buffer into the socket buffer

# This involves two context switches (read, write) which make a second copy of the same 
# object unnecessary. As you may see, it is not the optimal way. Thankfully, there is 
# another system call that improves sending files, and it's called (surprise, surprise!): 
# sendfile(2). This call retrieves an object to the file cache, and passes the pointers 
# (without copying the whole object) straight to the socket descriptor. Netflix states 
# that using sendfile(2) increased the network throughput from 6Gbps to 30Gbps.

# However, sendfile(2) has some drawbacks:
# * does not work with UNIX sockets (e.g. when serving static files through your upstream server)
# * can perform differently depending on the operating system

sendfile on;
tcp_nopush on;

# tcp_nopush is opposite to tcp_nodelay. Instead of pushing packages as fast as possible, 
# it aims to optimise the amount of data sent simultaneously. It will force the package 
# to wait until it gets its maximum size (MSS) before sending it to the client. This 
# directive only works, when sendfile is on.

# It may appear that tcp_nopush and tcp_nodelay are mutually exclusive. But if all 3 directives 
# are turned on, nginx will:
# * ensure packages are full before sending them to the client
# * for the last packet, tcp_nopush will be removed, allowing TCP to send it immediately, 
# without the 200ms delay

worker_process auto;
# The worker_process directive defines how many workers should be run. By default, 
# this value is set to 1. The safest setting is to use the number of cores by passing auto.

# But still, due to Nginx's architecture, which handles requests blazingly fast, 
# we probably won’t use more than 2-4 processes at a time (unless you are hosting Facebook, 
# or doing some CPU-intensive stuff inside nginx).

worker_connections 1024;
# This directive is directly related to worker_process is worker_connections. It specifies 
# how many connections can be opened by a worker process at the same time. This number includes 
# all connections (e.g. connections with proxied servers), and not only connections with clients. 
# Also, it is worth keeping in mind that one client can open multiple connections to fetch other 
# resources simultaneously.

worker_rlimit_nofile 2048;
# “Everything is a file” in Unix-based systems. It means that documents, directories, pipes, 
# or even sockets are files. The system has a limitation how many files can be opened simultaneously 
# by a process. To check the limits:

# ulimit -Sn      # soft limit
# ulimit -Sn      # hard limit

# This system limit must be tweaked in accordance with worker_connections. Any incoming connection 
# opens at least one file (usually a two-connection socket and either a backend connection socket 
# or a static file on disk). So it is safe to have this value equal to worker_connections * 2. 
# Nginx, fortunately, provides an option of increasing this system value within the nginx config. 
# To do so, add the worker_rlimit_nofile directive with a proper number and reload the nginx.

gzip on;               # enable gzip
gzip_http_version 1.1; # turn on gzip for http 1.1 and higher
gzip_disable "msie6";  # IE 6 had issues with gzip
gzip_comp_level 5;     # inc compresion level, and CPU usage
gzip_min_length 100;   # minimal weight to gzip file
gzip_proxied any;      # enable gzip for proxied requests (e.g. CDN)
gzip_buffers 16 8k;    # compression buffers (if we exceed this value, disk will be used instead of RAM)
gzip_vary on;          # add header Vary Accept-Encoding (more on that in Caching section)

# define files which should be compressed
gzip_types text/plain;
gzip_types text/css;
gzip_types application/javascript;
gzip_types application/json;
gzip_types application/vnd.ms-fontobject;
gzip_types application/x-font-ttf;
gzip_types font/opentype;
gzip_types image/svg+xml;
gzip_types image/x-icon;
# Enabling gzip should significantly reduce the weight of your response, thus it will appear faster 
# on the client side. Gzip has the different level of compressions: from 1 to 9. Incrementing this 
# level will reduce the size of the file, but also increase resources consumption. As a standard, 
# we keep this number between 3 and 5, because increasing the number will give very small gains but 
# significantly increase the CPU usage.

gzip_http_version 1.1;
# This directive tells nginx to use gzip only for HTTP 1.1 and above. We don’t include HTTP 1.0 here, 
# because for the 1.0 version, it is impossible to use both keepalive and gzip. You need to decide 
# which one you prefer: HTTP 1.0 clients missing out on gzip or HTTP 1.0 clients missing out on keepalive.

location ~* \.(jpg|jpeg|png|gif|ico|css|js)$ {
  expires 1M;
  add_header Cache-Control public;
  add_header Pragma public;
  add_header Vary Accept-Encoding;
}
# Caching is another thing that can speed up requests nicely for returning users.
# Managing the cache can be controlled just by two headers:
# * Cache-Control for managing cache in HTTP/1.1
# * Pragma for backward compatibility with HTTP/1.0 clients
# Vary Accept-Encoding; tells a public cache that a resource can be distinguished by a URI and an 
# Accept-Encoding header.

# Configure timeouts
client_body_timeout   12;
client_header_timeout 12;
send_timeout          10;
# The client_body_timeout and client_header_timeout define how long nginx should wait for a client to 
# transmit the body or header before throwing the 408 (Request Time-out) error. send_timeout sets a 
# timeout for transmitting a response to the client. The timeout is set only between two successive 
# write operations, not for the transmission of the whole response. If the client does not receive 
# anything within this time, the connection is closed.
# Be careful when setting those values, as too long waiting times can make you vulnerable to attackers, 
# while too short times will cut off slow clients.

client_body_buffer_size       16K;
client_header_buffer_size     1k;
large_client_header_buffers   2 1k;
client_max_body_size          8m;
# Sets the buffer size for reading the client's request body. In case the request body is larger than 
# the buffer, the whole body or only its part is written to a temporary file. For client_body_buffer_size, 
# setting 16k is enough in most cases. This is yet another setting that can have a massive impact, but 
# it has to be used with care. Put too little, and nginx will constantly use I/O to write remaining parts 
# to the file. Put too much, and you will make yourself vulnerable to DOS attacks when the attacker could 
# open all the connections, but you are not able to allocate a buffer on your system to handle those 
# connections. 

# If headers don’t fit into client_header_buffer_size then large_client_header_buffers will be used. If 
# the request also won’t fit into that buffer, an error is returned to the client. For most requests, a 
# buffer of 1K bytes is enough. However, if a request includes long cookies, it may not fit into 1K. If 
# the size of a request line is exceeded, the 414 (Request-URI Too Large) error is returned to the client. 
# If the size of a request header is exceeded, the 400 (Bad Request) error is thrown.

# Sets the maximum allowed size of the client request body, specified in the “Content-Length” request 
# header field. Depending on whether you want to allow users to upload files, tweak this configuration 
# to your needs.

# Nginx provides a few directives you can use to tweak keepalive settings. Those can be grouped into two 
# categories:

# keepalive between client and nginx
keepalive_disable msie6;        # disable selected browsers.

# The number of requests a client can make over a single keepalive connection. The default is 100, but a much higher value can be especially useful for testing with a load‑generation tool, which generally sends a large number of requests from a single client.
keepalive_requests 100000;

# How long an idle keepalive connection remains open.
keepalive_timeout 60;

# keepalive between nginx and upstream
upstream backend {
    # The number of idle keepalive connections to an upstream server that remain open for each worker process
    keepalive 16;
}

server {
  location /http/ {
    proxy_pass http://http_backend;
    proxy_http_version 1.1;
    proxy_set_header Connection "";
  }
}
# The TCP protocol, on which HTTP is based, requires performing a three-way handshake to initiate the 
# connection. It means that before the server can send you data (e.g. images), three full roundtrips between 
# the client and the server need to be made. For another request, you will have to perform this whole 
# initialisation once again. If you are sending multiple requests over short periods of time, this can add 
# up fast. And this is where keepalive comes in handy. After the successful response, it keeps the connection 
# idle for a given amount of time (e.g. 10s). If another request is made during this time, the existing 
# connection will be reused and the idle time refreshed.

